{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow"
      ],
      "metadata": {
        "id": "wf7p80fHX209"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CruWIG20XxAN",
        "outputId": "514bce45-6d28-4cf4-a152-44a1454e7025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7b7a7ffc8040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7b7a7ffc8040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Loss: 0.031173670664429665, Val Loss: 0.0770307406783104\n",
            "Epoch 100, Loss: 0.020884672179818153, Val Loss: 0.06462923437356949\n",
            "Epoch 150, Loss: 0.018347691744565964, Val Loss: 0.05752677097916603\n",
            "Epoch 200, Loss: 0.01745333895087242, Val Loss: 0.052449267357587814\n",
            "Epoch 250, Loss: 0.017068713903427124, Val Loss: 0.04848457872867584\n",
            "Epoch 300, Loss: 0.01691407337784767, Val Loss: 0.04532994329929352\n",
            "Epoch 350, Loss: 0.01686173304915428, Val Loss: 0.042758192867040634\n",
            "Epoch 400, Loss: 0.016857391223311424, Val Loss: 0.04061119630932808\n",
            "Epoch 450, Loss: 0.016903547570109367, Val Loss: 0.038820330053567886\n",
            "Epoch 500, Loss: 0.016953852027654648, Val Loss: 0.03728467598557472\n",
            "Test Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert datasets to TensorFlow tensors\n",
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
        "\n",
        "# Define the neural network structure\n",
        "class IrisNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(IrisNet, self).__init__()\n",
        "        self.fc1 = tf.keras.layers.Dense(12, activation='relu')  # First hidden layer with 12 neurons\n",
        "        self.fc2 = tf.keras.layers.Dense(8, activation='relu')   # Second hidden layer with 8 neurons\n",
        "        self.fc3 = tf.keras.layers.Dense(3)    # Output layer with 3 neurons (for the 3 classes)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc1(x)  # Apply ReLU activation function after first hidden layer\n",
        "        x = self.fc2(x)  # Apply ReLU activation function after second hidden layer\n",
        "        return self.fc3(x)  # No activation here as we'll use SparseCategoricalCrossentropy from logits\n",
        "\n",
        "# Initialize the model\n",
        "model = IrisNet()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 16\n",
        "epochs = 500\n",
        "patience = 10\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffling & batching the data helps the model to generalize better\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        x_batch = X_train[i:i + batch_size]\n",
        "        y_batch = y_train[i:i + batch_size]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch)\n",
        "            loss = loss_object(y_batch, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Validation loss\n",
        "    val_predictions = model(X_test)\n",
        "    val_loss = loss_object(y_test, val_predictions)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0  # Reset the counter after saving best loss\n",
        "    else:\n",
        "        counter += 1\n",
        "\n",
        "    if counter >= patience:\n",
        "        print(\"Early stopping due to no improvement!\")\n",
        "        break\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.numpy()}, Val Loss: {val_loss.numpy()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model(X_test)\n",
        "predicted_classes = tf.argmax(predictions, axis=1)\n",
        "accuracy = accuracy_score(y_test.numpy(), predicted_classes.numpy())\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the model\n",
        "model.save(\"iris_model\")\n",
        "\n",
        "# Load the model for inference\n",
        "loaded_model = tf.keras.models.load_model(\"iris_model\")\n",
        "\n",
        "# Make a prediction on new data\n",
        "new_data = [[5.1, 3.5, 1.4, 0.2], [6.7, 3.0, 5.2, 2.3]]  # Some iris measurements\n",
        "new_data = tf.convert_to_tensor(new_data, dtype=tf.float32)  # Convert the data to a TensorFlow tensor\n",
        "predictions = loaded_model(new_data)\n",
        "predicted_classes = tf.argmax(predictions, axis=1)\n",
        "print(predicted_classes.numpy())  # This will give you the indices of the predicted classes for each input"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MDFKoiIUYem0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}