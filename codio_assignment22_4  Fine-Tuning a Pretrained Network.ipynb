{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1af27d3e65f74335",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Required Codio Assignment 22.4: Fine-Tuning a Pretrained Network\n",
    "\n",
    "**Expected Time = 60 minutes**\n",
    "\n",
    "**Total Points = 30**\n",
    "\n",
    "In addition to the use of a pre-trained network to extract features from a different dataset, the weights can be adjusted or **fine-tuned** as a last step to squeeze additional performance from the network.  To do so, you will again use the `EfficientNetV2B0` network on the `cifar10` data from `keras`.  This time you are encouraged to use the functional API syntax to construct your network.  \n",
    "\n",
    "For a second example, consult the `keras` documentation example [here](https://keras.io/guides/transfer_learning/). \n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "\n",
    "Run the code cell below to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B0\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "sub_indices = pd.DataFrame(y_train, columns= [\"labels\"]).groupby('labels', group_keys=False).apply(lambda x: x.sample(frac=0.3, random_state=123)).index\n",
    "X_train = X_train[sub_indices]\n",
    "y_train = y_train[sub_indices]\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8d37aca8ee95d87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Training the Network to Convergence\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "In the code cell below, use the function `EfficientNetV2B0` with the appropriate input shape and the argument `include_top` equal to `False` to create the model you will be using for this activity. Assign your result to `base_model`.\n",
    "\n",
    "Use the function `Input()` with argument `shape` equal to `(32, 32, 3)` and assign your result to the variable `inputs`.\n",
    "\n",
    "Use `base_model` with argument equal to `inputs` and assign your result to the variable `x`.\n",
    "\n",
    "Use the function `Flatten` to flatten `x`. The pseudocode to complete this step is given below:\n",
    "\n",
    "```Python\n",
    "x = Flatten()(...)\n",
    "```\n",
    "\n",
    "Pass `x` through a `Dense` layer with 100 hidden nodes and `activation` equal to `relu` and assign the result to the variable `output`. The pseudocode to complete this step is given below:\n",
    "\n",
    "```Python\n",
    "output = Dense(..., activation = ...)(...)\n",
    "```\n",
    "\n",
    "Use the code `base_model.trainable = False` to ensure that your weights are not trainable.\n",
    "\n",
    "Use the function `Model()` with argument `inputs` and `output` to define your model. Assign the result to the variable `model`.\n",
    "\n",
    "Compile `model` using `categorical_crossentropy` as your `loss` and `accuracy` as your `metric`.\n",
    "\n",
    "Use the `fit()` function on `model` to fit the  training data `X_train` and `Y_train`. Set the argument `validation_data` equal to `(X_test, Y_test)` and the argument `epochs` equal to 1.  Assign the result to the variable `bottom_model` below. \n",
    "\n",
    "NOTE: This question is computationally expensive, so please be patient with the processing. It may take a few minutes based on your computing power. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e92e20d0c85d3026",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "base_model = ''\n",
    "inputs = ''\n",
    "x = ''\n",
    "x = ''\n",
    "x = ''\n",
    "output = ''\n",
    "model = ''\n",
    "#be sure to compile\n",
    "\n",
    "bottom_model = ''\n",
    "    \n",
    "### BEGIN SOLUTION\n",
    "base_model = EfficientNetV2B0(input_shape = (32, 32, 3), include_top=False)\n",
    "inputs = Input(shape = (32, 32, 3))\n",
    "x = base_model(inputs)\n",
    "x = Flatten()(x)\n",
    "x = Dense(10, activation = 'relu')(x)\n",
    "output = Dense(10, activation = 'sigmoid')(x)\n",
    "base_model.trainable = False\n",
    "model = Model(inputs, output)\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "bottom_model = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 1)\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72d9644d3c68011b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Setting to Trainable\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "In the code cell below, use the code `base_model.trainable = True` to ensure that your weights are now trainable.\n",
    "\n",
    "\n",
    "Set the final five layers to trainable in the `base_model` as demonstrated in the lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable #are the base model weights set to trainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-17e4f0a4de6dc3b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "base_model.trainable = True\n",
    "for layer in '':\n",
    "    #make trainable\n",
    "    pass\n",
    "    \n",
    "### BEGIN SOLUTION\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "for i, layer in enumerate(base_model.layers[-10:]):\n",
    "    print(f'Layer is trainable: {layer.trainable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0307a936396eedc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Refitting the network\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "In the code cell below, use the function `compile` on `model` using `categorical_crossentropy` as your `loss` and `accuracy` as your `metric`.\n",
    "\n",
    "Next, use the `fit()` function on `model` to fit the  training data `X_train` and `Y_train`. Set the argument `validation_data` equal to `(X_test, Y_test)` and the argument `epochs` equal to 1.  Assign the result to the variable `fine_tuned_history` below. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75f43fbafe9c0840",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "fine_tuned_history = ''\n",
    "    \n",
    "### BEGIN SOLUTION\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "fine_tuned_history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = 1)\n",
    "### END SOLUTION\n",
    "\n",
    "### ANSWER CHECK\n",
    "fine_tuned_history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
